RNN(Recurrent Neural Network)


순환신경망이라고 하며, 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델
  • 시퀀스란 연관된 연속 데이터를 의미하며 시계열 데이터 등 순서를 가지는 데이터에 적합한 신경망 모델
  • 예: 번역기(입력은 단어가 순서를 띄고 나열된 문장 시퀀스이며 출력도 동일), 주식 차트 등
가장 기본적인 시퀀스 모델이 RNN이며 이를 기반으로 개선된 LSTM과 GRU, Transformer 모델이 많이 사용된다.

RNN 기본 구조
ANN, DNN, CNN은 모두 은닉층에서 활성화함수를 지난 값은 출력층 방향으로만 적용된다.
  • Feed Forward Neural Network
이에 반해, RNN은 은닉층의 노드에서 활성화함수를 통해 나온 결과값을 출력층 방향 + 은닉층 노드의 다음 계산의 입력으로 보낸다.
  • x 입력을 받아서 h 은닉 상태를 통과하여 y 출력을 한다고 하면 다음과 같은 수식으로 동작한다.
    • x 입력과 h 간의 가중치 w_xh와, h와 y 간의 최적화를 위한 w_hy 가중치 이외에
    • h_t-1에서 h_t로의 계산을 위한 w_hh 가중치도 있음
    • h는 일반적인 wx + b가 아닌 tanh 활성화함수(비선형)를 사용함
기존 hidden state의 활성화함수를 통해 나온 결과값을 내보내는 역할을 하는 노드를 메모리 셀이라고 한다.
RNN은 은닉층 메모리 셀에서 나온 값을 다음 학습(h_t)때 메모리 셀에 입력하는 구조 = 플립플롭 논리회로와 유사

tanh 함수(Hyperbolic Tangent Function)
쌍곡선 함수라고도 하며 시그모이드 함수와 유사하여 대체제로 사용됨
  • 시그모이드는 0~1의 값을 가지는데 반해, tanh 함수는 -1~1의 값을 가진다.
  • 시그모이드는 데이터 평균이 0.5이지만, tanh 함수는 평균이 0이다.
  • 시그모이드와 비교하여 tanh 함수가 보다 출력 범위가 넓고 경사면이 더 깊숙하므로 더 빠르게 수렴하여 학습한다.

RNN의 다양한 구조
RNN은 입력과 출력 길이를 다르게 설계할 수 있다.
  • one to one: 가장 단순한 형태, Vanilla RNN이라고도 부름
  • one to many: 하나의 이미지 입력에 대해 이미지의 다양한 특징을 출력하는 이미지 캡셔닝에 적용 가능
  • many to one: 메일 전체를 입력받아 메일이 스팸인지, 스팸이 아닌지를 판단하는 등의 문제에 적용 가능
  • many to many: 문장을 입력받아 문장을 출력하는 챗봇이나 번역기 등의 문제에 적용 가능
