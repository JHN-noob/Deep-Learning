{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d6da4a-b2d7-4e89-b1a5-c9b86d494ecf",
   "metadata": {},
   "source": [
    "# PyTorch와 RNN, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d7dbf3-b9fe-40e1-8f38-0515d1ee7ce8",
   "metadata": {},
   "source": [
    "# RNN 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4361e77-26b6-450d-823f-e646f8e99ab3",
   "metadata": {},
   "source": [
    "# torch.nn.RNN\n",
    "torch.nn.RNN(args, *kwargs)\n",
    "- input_size: Input 크기(feature 수)\n",
    "- hidden_size: hidden state 크기\n",
    "  - DNN의 hidden layer의 사이즈라고 생각하면 된다. 즉, 노드 수\n",
    "- num_layers: 순환 레이서 수(Default: 1)\n",
    "  - 레이어 수, Multi-layer로 LSTM을 구성할 수 있지만 4개 초과부터는 Gradient Vanishing 이슈가 있다.\n",
    "- nonlinearity: 비선형 활성화함수 설정, 'tanh' 또는 'relu'(Default: 'tanh')\n",
    "- bias: bias 값 활성화 여부 설정(Default: True)\n",
    "- batch_first: True일 시, Output 사이즈를 (batch, seq, feature)로 출력\n",
    "  - Default: (seq, batch, feature)\n",
    "- dropout: 드롭아웃 비율 설정(Default: 0)\n",
    "- bidirectional: True일 시, Bidirectional RNN 적용(Default: False)\n",
    "\n",
    "#### Inputs: input, h_0 (tuple 형태)\n",
    "- input: 입력 텐서 - (sequence_length, batch_size, input_size) # batch_first가 True면 (batch_size, sequence_length, input_size)\n",
    "  - seqeunce_length: 노드를 얼마만큼 순환할지에 대한 값\n",
    "- h_0: hidden stats의 초기값 텐서 - (num_layers * bidirections, batch_size, hidden_size) 형태\n",
    "  - bidirectional이 True면 2, False면 1\n",
    "\n",
    "#### Outputs: output, h_n (tuple 형태)\n",
    "- output: 마지막 레이어의 출력 텐서 - (sequence_length, batch_size, bidrections * hidden_size)\n",
    "  - bidirectional이 True면 2, Fasle면 1\n",
    "- h_n: 마지막 hidden state 텐서 - (num_layers * bidirections, batch_size, hidden_size)\n",
    "  - bidirectional이 True면 2, Fasle면 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5e961-e56e-4f1e-9598-b76e501939fa",
   "metadata": {},
   "source": [
    "# Input 사이즈 및 하이퍼 파라미터 설정\n",
    "28 dimention(가로) x 28 sequence(세로)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892cccd-dbb7-49a2-baa4-fb34c9694a32",
   "metadata": {},
   "source": [
    "# input, sequence, 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c53270-8ef6-4f0d-855a-0b2780974777",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "feature_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "dropout_p = 0.2\n",
    "output_size = 10\n",
    "minibatch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a6180-6e0c-4ea0-8e22-f47924e6d648",
   "metadata": {},
   "source": [
    "# LSTM 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc98af1d-99dd-4282-b64e-a91d095a333f",
   "metadata": {},
   "source": [
    "# torch.nn.LSTM\n",
    "torch.nn.LSTM(args, *kwargs)\n",
    "- input_size: Input 크기(feature 수)\n",
    "- hidden_size: hidden state 크기\n",
    "  - DNN의 hidden layer의 사이즈라고 생각하면 된다. 즉, 노드 수\n",
    "- num_layers: 순환 레이서 수(Default: 1)\n",
    "  - 레이어 수, Multi-layer로 LSTM을 구성할 수 있지만 4개 초과부터는 Gradient Vanishing 이슈가 있다.\n",
    "- bias: bias 값 활성화 여부 설정(Default: True)\n",
    "- batch_first: True일 시, Output 사이즈를 (batch, seq, feature)로 출력\n",
    "  - Default: (seq, batch, feature)\n",
    "- dropout: 드롭아웃 비율 설정(Default: 0)\n",
    "- bidirectional: True일 시, Bidirectional LSTM 적용(Default: False)\n",
    "\n",
    "#### Inputs: input, (h_0, c_0)\n",
    "- input: (sequence_length, batch_size, input_size) # batch_first가 True면 (batch_size, sequence_length, input_size)\n",
    "  - seqeunce_length: 노드를 얼마만큼 순환할지에 대한 값\n",
    "- h_0: (num_layers * bidirections, batch_size, hidden_size)\n",
    "  - bidirectional이 True면 2, False면 1\n",
    "- c_0: (num_layers * bidirections, batch_size, hidden_size)\n",
    "  - bidirectional이 True면 2, False면 1\n",
    "\n",
    "#### Outputs: output, (h_n, c_n)\n",
    "- output: (sequence_length, batch_size, bidrections * hidden_size)\n",
    "  - bidirectional이 True면 2, Fasle면 1\n",
    "- h_n: (num_layers * bidirections, batch_size, hidden_size)\n",
    "  - bidirectional이 True면 2, Fasle면 1\n",
    "- c_n: (num_layers * bidirections, batch_size, hidden_size)\n",
    "  - bidirectional이 True면 2, Fasle면 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b28ed-d0e3-4a10-b5ee-90937dc188ed",
   "metadata": {},
   "source": [
    "# RNN/LSTM 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec65b60-f6ba-46a4-8f65-30f1cbda7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beaaa471-fb48-4bd0-9fde-9c389da8e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 28\n",
    "feature_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 4\n",
    "dropout_p = 0.2\n",
    "output_size = 10\n",
    "minibatch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "665064aa-1818-419c-9821-9d4c90992215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size, num_layers, dropout_p, output_size, model_type):\n",
    "        super().__init__()\n",
    "        if model_type == 'rnn':\n",
    "            self.sequenceclassfier = nn.RNN(\n",
    "                input_size = feature_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = dropout_p,\n",
    "                bidirectional = True\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.sequenceclassfier = nn.LSTM(\n",
    "                input_size = feature_size,\n",
    "                hidden_size = hidden_size,\n",
    "                num_layers = num_layers,\n",
    "                batch_first = True,\n",
    "                dropout = dropout_p,\n",
    "                bidirectional = True\n",
    "            )\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm1d(hidden_size * 2),\n",
    "            # rnn 및 lstm의 출력값은 (sequence_length, batch_size, bidrections * hidden_size)\n",
    "            # bidirectional이 True이므로 hidden_size * 2\n",
    "            nn.Linear(hidden_size * 2, output_size),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # |x| = batch_first = True이므로 (batch_size, sequence_length, input_size)\n",
    "        out, _ = self.sequenceclassfier(x)\n",
    "        # output, h_n이므로, h_n은 사용안함\n",
    "        # |out| = batch_first = True이로로 (batch_size, sequence_length, 2 * hidden_size)\n",
    "        # bidirectional이 True면 bidirections는 2 * hidden_size\n",
    "        out = out[:, -1]\n",
    "        # out[:, -1]은 (batch_size, sequence_length, 2 * hidden_size)에서,\n",
    "        # 전체 batch_size를 선택한다는 의미의 :,\n",
    "        # sequence_length에는 28개의 순서가 있고 각 순서마다 2 * hidden_size만큼 있다.\n",
    "        # 이중에서 최종값은 마지막 sequence_length의 2 * hidden_size이다.\n",
    "        # |out| = (batch_size, 2 * hidden_size)\n",
    "        y = self.layers(out)\n",
    "        # |out| = (batch_size = 128, output_size = 10)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00b4994-c109-4515-b6b3-c60dbb229bec",
   "metadata": {},
   "source": [
    "# 참고 코드: shape와 slicing 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52ea33b-26ea-47b7-9ce2-ab923e396ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 28, 256]) torch.Size([128, 256])\n",
      "torch.Size([128, 1, 28, 28]) torch.Size([128, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "data1 = torch.full((minibatch_size, sequence_length, 2 * hidden_size), 1) # 3D tensor 생성\n",
    "data2 = data1[:, -1]\n",
    "print(data1.shape, data2.shape)\n",
    "\n",
    "data3 = torch.full((minibatch_size, 1, sequence_length, feature_size), 1) # 4D tensor 생성\n",
    "data4 = data3.reshape(-1, sequence_length, feature_size)\n",
    "print(data3.shape, data4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f327db-ff02-4b36-af36-d05f5bc7abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (sequenceclassfier): RNN(28, 128, num_layers=4, batch_first=True, dropout=0.2, bidirectional=True)\n",
       "  (layers): Sequential(\n",
       "    (0): LeakyReLU(negative_slope=0.1)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Linear(in_features=256, out_features=10, bias=True)\n",
       "    (3): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(feature_size, hidden_size, num_layers, dropout_p, output_size, 'rnn')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693312fc-8581-405a-b748-7599be5c3099",
   "metadata": {},
   "source": [
    "# MNIST with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638bb37b-8af9-421b-aaae-727385134f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data :  60000\n",
      "number of test data :  10000\n"
     ]
    }
   ],
   "source": [
    "train_rawdata = datasets.MNIST(root = 'dataset_MNIST',\n",
    "                               train = True, # True면 Train 데이터\n",
    "                               download = True, # 데이터가 없으면 Download\n",
    "                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌\n",
    "test_rawdata = datasets.MNIST(root = 'dataset_MNIST',\n",
    "                               train = False, # False면 Test 데이터\n",
    "                               download = True, # 데이터가 없으면 Download\n",
    "                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌\n",
    "print('number of training data : ', len(train_rawdata))\n",
    "print('number of test data : ', len(test_rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14a13a29-3a26-4596-8f93-fabcb24341be",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATE = 0.2\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_rawdata)), # Train 데이터셋의 인덱스 번호 추출(0~59999)\n",
    "    train_rawdata.targets, # y 정답 라벨\n",
    "    stratify = train_rawdata.targets, # y 정답 라벨 균등분포\n",
    "    test_size = VALIDATION_RATE # 여기선 Validation 데이터셋 비율\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5ad1d06-d974-43b8-987e-d37a57d2b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_rawdata, train_indices)\n",
    "validation_dataset = Subset(train_rawdata, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9651225e-0645-4b0e-96ff-ed048c7b2cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(validation_dataset), len(test_rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a048297b-ce61-4937-9479-fa1bd79c93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_batchs = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "va_batchs = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_batchs = DataLoader(test_rawdata, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d11393-82f8-4ca8-ae0e-bfe3946e9831",
   "metadata": {},
   "source": [
    "# input, output, loss, optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c180f8e0-fc5a-4b29-a117-a588ca6fff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f98d7f-a104-4731-b8d4-3c69ef9d44fa",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "697ee3b5-2a06-40f7-962a-145a286b64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, early_stop, nb_epochs, progress_interval):\n",
    "    train_losses, valid_losses, lowest_loss = list(), list(), np.inf\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        train_loss, valid_loss = 0, 0\n",
    "\n",
    "        # train model\n",
    "        model.train() # prepare model for training\n",
    "        for x_minibatch, y_minibatch in train_batchs:\n",
    "            x_minibatch = x_minibatch.reshape(-1, sequence_length, feature_size)\n",
    "            y_minibatch_pred = model(x_minibatch)\n",
    "            loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_batchs)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validate model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_minibatch, y_minibatch in va_batchs:\n",
    "                x_minibatch = x_minibatch.reshape(-1, sequence_length, feature_size)\n",
    "                y_minibatch_pred = model(x_minibatch)\n",
    "                loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "                valid_loss += loss.item()\n",
    "    \n",
    "        valid_loss = valid_loss / len(va_batchs)\n",
    "        valid_losses.append(valid_loss)\n",
    "    \n",
    "        if valid_losses[-1] < lowest_loss:\n",
    "            lowest_loss = valid_losses[-1]\n",
    "            lowest_epoch = epoch\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            if (early_stop > 0) and lowest_epoch + early_stop < epoch:\n",
    "                print(\"Early Stopped\", epoch, \"epochs\")\n",
    "                break\n",
    "    \n",
    "        if (epoch % progress_interval) == 0:\n",
    "            print(train_losses[-1], valid_losses[-1], lowest_loss, lowest_epoch, epoch)\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, lowest_loss, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808656f-e767-499b-8425-56478238574f",
   "metadata": {},
   "source": [
    "# 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75a2b25a-ead6-48f5-84fc-2d238b9a43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.560015678524971 0.3442645991577747 0.3442645991577747 0 0\n",
      "0.154399822473526 0.15432094045458955 0.15432094045458955 3 3\n",
      "0.12376958649853866 0.13316993526321777 0.10539618796332086 5 6\n",
      "0.11104683033873637 0.10592339677300225 0.09711978968946224 7 9\n",
      "0.0937984434440732 0.1223241393355296 0.09711978968946224 7 12\n",
      "Early Stopped 13 epochs\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 30\n",
    "progress_interval = 3\n",
    "early_stop = 5\n",
    "\n",
    "model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, nb_epochs, progress_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe32ab6-85fe-4de1-96f9-f6ee78cfee42",
   "metadata": {},
   "source": [
    "# 테스트셋 기반 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c150872a-233d-47af-af38-169eb28cafa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 0.0007\n",
      "Accuracy: 9726/10000 (97.26%)\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "wrong_samples, wrong_preds, actual_preds = list(), list(), list()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_minibatch, y_minibatch in test_batchs:\n",
    "        x_minibatch = x_minibatch.reshape(-1, sequence_length, feature_size)\n",
    "        y_test_pred = model(x_minibatch)\n",
    "        test_loss += loss_func(y_test_pred, y_minibatch)\n",
    "        pred = torch.argmax(y_test_pred, dim=1)\n",
    "        correct += pred.eq(y_minibatch).sum().item()\n",
    "\n",
    "        wrong_idx = pred.ne(y_minibatch).nonzero()[:, 0].numpy().tolist()\n",
    "        for index in wrong_idx:\n",
    "            wrong_samples.append(x_minibatch[index])\n",
    "            wrong_preds.append(pred[index])\n",
    "            actual_preds.append(y_minibatch[index])\n",
    "\n",
    "test_loss /= len(test_batchs.dataset)\n",
    "print('Average Test Loss: {:.4f}'.format(test_loss))\n",
    "print('Accuracy: {}/{} ({:.2f}%)'.format(correct, len(test_batchs.dataset), 100*correct/len(test_batchs.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
