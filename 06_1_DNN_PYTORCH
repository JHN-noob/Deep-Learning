import torch
import torch.nn as nn


x = torch.ones(5000, 10)
y = torch.zeros(5000, 1)
learning_rate = 0.01
nb_epochs = 1000
minibatch_size = 256

input_dim = x.size(-1)
output_dim = y.size(-1)

model = nn.Sequential(
    nn.Linear(input_dim, 10),
    nn.LeakyReLU(0.1),
    nn.Linear(10, 8),
    nn.LeakyReLU(0.1),
    nn.Linear(8, 6),
    nn.LeakyReLU(0.1),
    nn.Linear(6, output_dim)
)

loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

for index in range(nb_epochs): # 에포크만큼 반복(1000번)
    indices = torch.randperm(x.size(0)) # 5000개의 인덱스 번호를 만들어서 섞음

    x_batch_list = torch.index_select(x, 0, index=indices)
    y_batch_list = torch.index_select(y, 0, index=indices)
    x_batch_list = x_batch_list.split(minibatch_size, dim=0) # 행을 기준으로 미니배치 사이즈로 나눈다
    y_batch_list = y_batch_list.split(minibatch_size, dim=0) # 행을 기준으로 미니배치 사이즈로 나눈다

    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list): # 미니배치 사이즈로 나눈 데이터들을 반복(20번)
        y_minibatch_pred = model(x_minibatch)
        loss = loss_function(y_minibatch_pred, y_minibatch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

print(loss)
for param in model.parameters():
    print(param)
