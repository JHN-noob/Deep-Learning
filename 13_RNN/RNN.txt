RNN(Recurrent Neural Network)


순환신경망이라고 하며, 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델
  • 시퀀스란 연관된 연속 데이터를 의미하며 시계열 데이터 등 순서를 가지는 데이터에 적합한 신경망 모델
  • 예: 번역기(입력은 단어가 순서를 띄고 나열된 문장 시퀀스이며 출력도 동일), 주식 차트 등
가장 기본적인 시퀀스 모델이 RNN이며 이를 기반으로 개선된 LSTM과 GRU, Transformer 모델이 많이 사용된다.

RNN 기본 구조
ANN, DNN, CNN은 모두 은닉층에서 활성화함수를 지난 값은 출력층 방향으로만 적용된다.
  • Feed Forward Neural Network
이에 반해, RNN은 은닉층의 노드에서 활성화함수를 통해 나온 결과값을 출력층 방향 + 은닉층 노드의 다음 계산의 입력으로 보낸다.
  • x 입력을 받아서 h 은닉 상태를 통과하여 y 출력을 한다고 하면 다음과 같은 수식으로 동작한다.
    • x 입력과 h 간의 가중치 w_xh와, h와 y 간의 최적화를 위한 w_hy 가중치 이외에
    • h_t-1에서 h_t로의 계산을 위한 w_hh 가중치도 있음
    • h는 일반적인 wx + b가 아닌 tanh 활성화함수(비선형)를 사용함
기존 hidden state의 활성화함수를 통해 나온 결과값을 내보내는 역할을 하는 노드를 메모리 셀이라고 한다.
RNN은 은닉층 메모리 셀에서 나온 값을 다음 학습(h_t)때 메모리 셀에 입력하는 구조 = 플립플롭 논리회로와 유사

tanh 함수(Hyperbolic Tangent Function)
쌍곡선 함수라고도 하며 시그모이드 함수와 유사하여 대체제로 사용됨
  • 시그모이드는 0~1의 값을 가지는데 반해, tanh 함수는 -1~1의 값을 가진다.
  • 시그모이드는 데이터 평균이 0.5이지만, tanh 함수는 평균이 0이다.
  • 시그모이드와 비교하여 tanh 함수가 보다 출력 범위가 넓고 경사면이 더 깊숙하므로 더 빠르게 수렴하여 학습한다.

RNN의 다양한 구조
RNN은 입력과 출력 길이를 다르게 설계할 수 있다.
  • one to one: 가장 단순한 형태, Vanilla RNN이라고도 부름
  • one to many: 하나의 이미지 입력에 대해 이미지의 다양한 특징을 출력하는 이미지 캡셔닝에 적용 가능
  • many to one: 메일 전체를 입력받아 메일이 스팸인지, 스팸이 아닌지를 판단하는 등의 문제에 적용 가능
  • many to many: 문장을 입력받아 문장을 출력하는 챗봇이나 번역기 등의 문제에 적용 가능


RNN의 학습, BPTT(Back Propagation Through Time)
RNN의 Back Propagation은 DNN과 동일하지만 hidden state를 따라서 이전 hidden state까지 역전파되므로 Back Propagation Through Time이라고 부른다.
손실함수를 정의하자면, Softmax와 Cross Entropy Loss를 사용하는 Classfication 모델이라면 H(P, Q) = -(P(n)log2Q(n)의 총합)
  • 즉, Loss(y_pred, y)는 -(y_tlog[softmax(o_t)]의 총합)이라고 할 수 있음, y_pred_t = softmax(o_t)
가중치 w_yh는 전 시간에 공유되므로, w_yh 기반으로 미분이 가능하다.
  • o_t = h_t * w_yh + b_y이고 이를 미분하면 h_t이므로 체인룰에 따라 L_t를 w_yh로 미분하면 h_t이다.
마찬가지로 L_t+1은 h_t+1과 전 시간에 공유되는 w_hh를 기반으로 다음과 같이 미분할 수 있다.
  • L_t+1을 w_hh로 미분
이를 h_t까지 확장하면 다음과 같이 미분할 수 있다.(h_t+1 계산에 h_t가 사용되므로 다음과 같이 미분 가능)
  • L_t+1을 w_hh로 미분하지만 중간에 h_t가 들어감
즉, h_t+1을 h_k로 미분하는건 체인룰이므로 예를 들어 다음과 같이 표현할 수 있다.
  • 결국 중간의 h_t+1은 h_t, h_t-1, ...h_k로 미분할 수 있다는 뜻이다.
따라서 L_t+1은 각 시점에서 발생한 hidden state까지의 미분값을 모두 합친것이다.
hidden state는 tanh 함수를 사용하고 tanh 함수의 미분은 0~1까지의 기울기가 나오기때문에, 미분값이 계속 곱해지면 결국 Gradient Vanishing이 발생한다.
이로 인해 긴 Sequence를 가진 데이터(Time-Step이 긴 데이터)는 가중치가 거의 업데이트되지않는 문제가 발생한다.(이를 긴 Sequence를 기억하지 못한다고 표현)


LSTM
RNN의 Gradient Vanishing 문제를 해결하기위해 나온 딥러닝 구조이다.
장기 의존성 문제를 해결하기 위해 셀 상태(Cell State) 구조 구성

Cell State
LSTM 핵심 아이디어로 기존의 정보를 저장하기 위한 역할을 한다.
  • 긴 Sequence라 할지라도 학습이 잘 진행되도록 하는 역할
Cell State에 망각(Forget), 입력(Input), 출력(Output) 게이트를 추가하여 불필요한 기억을 지우거나 기억해야할 데이터를 결정함

Forget Gate
과거 정보를 기억할지 잊을지를 결정하는 게이트
Sigmoid 활성화함수의 출력범위는 0~1이므로 h_t-1과 x_t를 보고 Cell State C_t-1의 각 데이터에 대해 0~1을 출력하여 과거 데이터 중 버릴 데이터 구분
  • 0이면 해당 데이터를 온전히 버리고, 1이면 해당 데이터를 온전히 기억
수식 : f_t ⓧ C_t-1
  • f_t = Sigmoid(w_f[h_t-1, x_t] + b_f)
  • ⓧ : 아마다르 곱, 각 행렬의 원소끼리만 곱한것
  • C_t-1을 Vector라고 생각하면 각 데이터마다 0~1의 값이 적용된다. 이전 셀에서 넘어온 정보 중에 중요하지않은 정보를 지우는 역할

Input Gate
현재 정보를 잊을지 기억할지를 결정하는 게이트
h_t-1(전 시점의 hidden state)과 x_t(현 시점의 입력)를 시그모이드 함수로 계산하여 업데이트할 데이터를 선택
  • i_t = Sigmoid(w_i[h_t-1, x_i] + b_i)
동일 입력을 tanh 함수를 적용하여 Cell State에 추가될 수 있는 새로운 후보값 C_pred 벡터 생성
  • C_pred = tanh(w_c[h_t-1, x_i] + b_c)
즉, 새로운 후보값 벡터 중 어느 정보를 현재 정보로 취할지를 결정하는것이다.

Cell State Update
i_t ⓧ C_pred를 통해 Cell State에 추가될 수 있는 새로운 데이터를 생성하고,
f_t ⓧ C_t-1을 통해 기존 Cell State 벡터 데이터 중 선별적으로 기억할 데이터를 선택
  • 수식 : C_t = (f_t ⓧ C_t-1) + (i_t ⓧ C_pred)
Cell State의 업데이트는 위 두값(input gate, forget gate 기반 계산)을 더하여 계산함
  • 곱셉이 아닌 덧셈이므로, 가중치 업데이트를 위한 역전파 시, Gradient Vanishing을 막을 수 있다.
    • 즉, L_t에 대해 w를 미분하는것은 중간에 C_t, C_t-1, C_t-2, ...C_1까지 포함되지만 이 부분은 덧셈이기 때문에 1이 넘는 값이 나올 수도 있는것이다.

Output Gate
셀 밖으로 내보낼 결과물 h_t를 출력하는 역할
h_t-1과 x_t를 시그모이드 함수로 계산하여 어떤 값을 hidden state로 전달하고, 또 output으로 내보낼지 결정한다.
  • 수식 : o_t = Sigmoid(w_o[h_t-1, x_t] + b_o)
  • h_t = o_t ⓧ tanh(C_t)


이외의 Approach: GRU(Gated Recurrent Unit)
LSTM과 같이 RNN 이슈를 해결하기위해 고안
GRU는 LSTM과 달리 Reset Gate와 Update Gate로 구성되며 Output Gate가 존재하지않음
  • Reset Gate는 이전 상태를 얼마나 반영할지 결정
  • Update Gate는 이전 상태와 현재 상태를 얼마만큼의 비율로 반영할지 결정
GRU가 계산량이 적다고 알려져있지만, 최근 연구에서는 LSTM이 보다 많이 사용됨


Bidirectional RNN
RNN은 시계열 데이터에 많이 사용됨
  • 이때 목표는 '지금까지 주어진것을 보고 다음것을 예측'하는것
하지만 RNN 계열 모델은 언어처리 분야에서도 많이 사용됨
  • 이때는 문장 앞부분뿐만 아니라, 문장 뒷부분의 데이터도 문장 가운데를 결정하는데 필요함
이를 반영하기위해 고안한 모델이 Bidirectional RNN
Bidirectional RNN은 RNN이 두개 쌓여있는 구조로,
  • 첫번째 RNN은 forward mode로 h_1 → h_t 방향으로 계산되고,
  • 두번째 RNN은 backward mode로 h_t ← h_1 방향으로 계산됨
