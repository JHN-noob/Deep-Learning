Key Models for Image


Vision Transformer

Sequence 2 Sequence 모델 요약
  • 기존 딥러닝 모델은 입력과 출력 차원이 모두 고정된 사이즈로 제한되는 문제를 가지고 있다.
  • 예를 들어, 번역모델을 만든다면 입력과 출력은 문장마다 다를 수 있다.
  • Sequence 2 Sequence 모델은 이를 해결하고자 한 모델이다.
  • Sequence 2 Sequence 모델 구조
    • 인코더와 디코더로 구성되며 내부에는 크게 2개의 RNN계열 모델 기반으로 구성됨
    • 인코더는 입력 문장은 순차적으로 입력받은 후 모든 단어 정보를 압축해서 Context Vector로 만든다.
    • 이를 디코더로 전송하여 디코더는 Context Vector를 기반으로 번역된 단어를 순서대로 출력한다.
  • Sequence 2 Sequence 모델 문제점
    • 입력 데이터가 매우 길다면 1. 이를 함축하여 Context Vector로 만들게 될 경우 정보 손실이 될 수 있음
    • 2. RNN계열의 근본적인 이슈는 Gradinet Vanishing 문제가 존재한다.
  • 이를 해결하기위해 나온 개념이 Attention

Attention 요약
  • 기본 아이디어
    • 디코더에서 출력 단어를 예측할때마다 인코더에서 들어온 전체 입력 문장을 참고하되,
    • 전체 입력 문장을 모두 동일 비율로 참고하는것이 아니라
    • 해당 시점에 예측해야할 단어와 관련된 입력 단어에 조금 더 높은 비율을 두어 참고하는것
    • 입력과 출력이 서로 대응되는 정보에 대해서는 유사한 hidden state 결과값을 가질것이라고 가정하기 때문이다.
  • Attention 동작 과정 요약
    • 인코더에서 각 입력마다 hidden state를 만든다.
    • 디코더의 출력때마다, 당시의 디코더의 hidden state와 인코더의 hidden state간의 유사도를 계산(유사한 입력과 출력 정보를 찾기위해)
    • 유사도가 높은 hidden state를 집중해서 반영하기위해 유사도 정도를 softmax 등을 통해 계산
      • 유사도가 높은 입력 정보만 집중(Attention)하는것이다.
    • 이를 기반으로 Context Vector를 만든다.
      • 즉, 모든 입력 정보를 압축하는것이 아니라 유사도가 높은 입력 정보를 중심으로 압축한다.
    • 이를 기반으로 디코더 출력값을 계산한다.
    • Attention 동작을 실제 구현하기위해 다양한 수학적 계산식이 제안되었다.

참고 : Attention Score 계산 예
  • Attention Score 계산
    • score(s_t, h_i) = s_t(전치) * h_i
      • t(time step)의 디코더 hidden state)를 s_t라고 정의
      • s_t(전치)는 디코더에서 현재 시점의 t의 은닉 벡터를 전치시킨것(왜냐하면 한개의 히든 스테이트(스텝)니까(예: (4, 1)))
      • 인코더의 t(time step) 시점의 인코더의 각 hidden state 1, 2, 3, ...N에 해당하는 은닉 상태(hidden state) 값을 각각 h_1, h_2, h_3, ...h_N이라고 정의
  • s_t와 인코더의 모든 hidden state(현재 시점 t)의 Attention Score의 모음 값을 e(t)로 정의하면,
    • e(t) = [s_t(전치) * h_1, s_t(전치) * h_2, s_t(전치) * h_3, ..., s_t(전치) * h_N]
  • 이를 softmax에 넣어, 각 hidden state의 어텐션 가중치(Attention Weight)를 구한다.
    • α(t) = softmax(e(t))
  • 각 어텐션 가중치와 hidden state 값을 곱하여 어텐션 값(Attention Value)를 구한다.
    • a_t = α_i(t) * h_i들의 합(i는 N)
    • 바로 이 a_t가 Context Vector가 된다.


Transformer 모델이란?
  • Attention Is All You Need 논문에서 제안되었음
  • Transformer는 Sequence 2 Sequence 모델과 유사하게 인코더와 디코더로 구성되지만
  • 내부에는 RNN계열 모델을 사용하지않고, N개(Attention Is All You Need에서는 6개)의 인코더와 디코더로 구성됨
  • Transformer 모델 구조
    • 논문상에서는 인코더와 디코더를 각각 6개의 레이어로 구성함
      • 각 레이어는 2개의 sub-layer로 구성

Positional Encoding
  • RNN계열 모델은 순서가 있는 입력을 처리할 수 있지만, Transformer는 입력 순서를 알려주기 어려우므로
  • 입력 정보에 위치 정보를 더해서 입력으로 사용함

Multi-Head Self-Attention과 Add & Norm
  • Self Attention
    • 입력 순차 데이터끼리 유사도를 구해서 Attention Score를 구하는 기법
    • 예를 들어, 입력 순차 데이터가 문장일 경우 문장 내의 단어들끼리 유사도를 계산하므로
    • 문장 내에서 it과 animal 단어가 연관되어있음을 찾아내어 예측에 반영이 가능하다.
  • Multi-Head Attention
    • 논문을 기준으로 예를 들면 512 차원의 단어 벡터를 num_heads 하이퍼 파라미터 값(논문은 8)으로 나누어
    • 512 / 8 = 64차원의 벡터를 기반으로 각각 Attention을 수행한다.
    • 문장이 하나가 아니라 여러개로 쪼개면 놓치는 부분없이 다양한 값(각각 가중치 값이 있기 때문)으로 Attention해야할 정보를 수집한다.
  • Add & Norm
    • 잔차 연결(Residual Connection)과 레이어 정규화(Layer Normalization)을 의미한다.
    • 잔차 연결
      • 잔차 연결은 모델의 학습을 돕는 기법으로 식은 다음과 같다.
      • Multi-Head Attension sub-layer의 입력값을 x, 출력값을 Multi_head Attention(x)라고 정의한다면
      • 잔차 연결식(H(x))는 H(x) = x + Multi_head Attention(x)
        • Transformer의 모든 layer 차수는 512로 동일하므로 덧셈이 가능하다.
    • 레이어 정규화
      • 잔차 연결값은 결국 행렬이므로 즉, 각각의 행(샘플 or 토큰 1개)에 대한 열의 분산과 평균을 구하여 정규화값으로 변환한다.
        • feature들을 정규화해도 되는 이유는 토큰(문장)이라 값의 분포가 다 유사하기 때문이며, BN은 배치(행)들을 묶어서 1개의 feature에 대해 정규화를 한다.
        • LN = LayerNorm(x + Sublayer(x))
      • 정규화 시, γ와 β를 추가해서 정규화값을 계산한다.(γ와 β는 오차역전파로 업데이트되는 값)
        • ln_i = γx_i + β = LyaerNorm(x_i)

Position-wise Feed Forward Neural Network
  • 기본적인 FC Layer이지만, 입력이 Positional Encoding을 하므로 Position마다 적용되는 FC Layer이다.

디코더의 첫번째 sub-layer의 Masked Multi-Head Attention
  • 디코더의 첫번째 sub-layer에 적용되는 Attention 기법
  • RNN계열 모델은 입력 데이터를 순차적으로 입력받으므로 현재까지 입력받은 입력 데이터에 한해서만 예측을 수행한다.
  • Transformer 모델은 전체 입력 데이터를 한꺼번에 입력받으므로 현재 시점에는 고려하지 않아야할 미래 데이터까지 고려해서 예측을 수행한다.
  • 따라서 이를 막기위해 현재보다 미래의 데이터는 보지못하도록 마킹을 하고 Self-Attention을 수행한다.

디코더의 두번째 sub-layer의 Encoder-Decoder Multi-Head Attention
  • 기본 Attention과 같이 디코더의 첫번째 sub-layer의 출력과 인코더의 마지막 layer에서 나온 hidden state들간의 Attention을 계산한다.
  • Sequence 2 Sequence에서 순차적으로 입력된 모든 hidden state들을 계산하듯, Transformer도 step별로 디코더의 출력이 입력으로 다시 들어가서 인코더의 hidden state들간의 Attention을 계산한다.


Vision Transformer
  • Step1 : 이미지를 고정된 patch 사이즈로 자른다.
  • Step2 : 2차원 행렬 픽셀 데이터를 1차원 데이터로 Flatten한다.
  • Step3 : Positional Encoding 적용 + CLS(예: 1 x 768 벡터)라는 각 잘려진 이미지를 분류할 수 있는 추가 정보 적용
  • Step4 :
    • Transformer Encoder는 Multi-Head self-Attention 수행(잔차 연결(Residual Connection)과 레이어 정규화(Layer Normalization)도 적용)
    • 이를 MLP 레이어(FC Layer)에 넣어서 출력결과를 얻는다.


Resnet
  • CNN 모델 깊이를 늘리면 늘릴수록 성능이 개선되는가? 에 대한 물음에서 시작
    • 56개의 레이어의 모델 성능이 20개의 레이어의 모델 성능보다 좋지않았다.
  • 주요 원인 중 하나로 Gradient Vanishing 현상을 지목함
  • Residual block을 기존 레이어에 추가하여 문제를 해결하고자 함
  • 기존 CNN 모델
    • x : 입력값
    • F(x) : CNN Layer → ReLU → CNN Layer를 통과한 출력값
    • H(x) : CNN Layer → ReLU → CNN Layer → ReLU를 통과한 출력값
    • H(x)가 정답 y에 가까워지도록, 결국 H(x)가 y를 출력하도록 만드는것이 목표
  • Resnet 모델
    • H(x)가 y가 아니라, x를 출력하도록 만드는것이 목표
    • H(x) = F(x) + x
    • 입력과 출력을 바로 연결하므로 shortcut이라고도 부르고 하나 이상의 레이어를 skip하는 효과가 있어 skip connection이라고도 함
    • 결국 몇몇 레이어를 건너뛰는 효과가 있어 Gradient Vanishing 문제를 완화한다.


EfficientNet
  • 기존 CNN계열 모델은 크기를 키움으로써 성능을 높이는 방향으로 연구가 많이 이루어졌다.
  • 모델을 크게 만드는 기법은 다음과 같이 3가지가 주요하다.
    • 네트워크 깊이를 보다 깊게
    • 네트워크 너비를 보다 넓게
    • 입력 이미지의 해상도를 높게
  • 각 모델은 위 3가지 방법 중 각각의 방법을 하나씩 선택하여 집중하는 경향이 있었다.
  • EfficientNet은 이 3가지 기법을 효율적으로 병합하여 모델을 확장하는 방법을 제안하였다.
  • 예시
    • 모델 크기를 2^^N배 키우고자 한다면,
    • 네트워크 깊이(network depth)를 α^^N배, 네트워크 너비(network width)를 β^^N배, 이미지 해상도(image size) γ배 늘린다.
    • 이를 위해 우선, α, β, γ는 작은 모델에 grid search를 실행하여 구한 최적값을 사용한다.
    • 이후, Φ값을 변화시켜서(예: 0~8), 각각 α^^Φ, β^^Φ, γ^^Φ배로 모델 크기를 크게 만든다.
      • Φ값 변화를 기반으로 Efficient-B0 ~ Efficient-B8 등과 같이 모델 크기를 키운 Pre-Trained 모델이 있다.
    • 이를 Compound Scaling이라고 한다.
