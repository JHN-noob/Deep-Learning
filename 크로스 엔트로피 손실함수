Cross Entropy Loss Function

두 확률분포의 차이를 계산하는 손실함수
실제 데이터의 분포를 원핫인코딩으로 표현했을 때와 모델이 계산한 확률의 차이를 계산한다.
H(P, Q) = -(P(n) * logQ(n)의 총합)
H(P, Q) = -((0 * log0.15) + (1 * log0.65) + (0 * log0.20))
  • 앞단에 -를 붙이는 이유는 0~1 사이의 log값은 음수이기 때문(양수로 변환)


Entropy와 Cross Entropy Loss

크로스 엔트로피는 이론적으로 P와 Q 두 확률간의 Entropy를 계산하는 식이다.
  • Entropy : 정보를 표현하는데 있어 필요한 최소 자원량
  • 정보 x를 비트로 표현하기 위해서는 비트 log2x개가 필요함(정보 4개면 비트 2개)
만약 특정 정보가 나올 확률이 확연히 높다면 해당 정보는 작은 비트수를 할당하고 낮은 정보는 큰 비트수를 할당한다.
각 정보가 나올 확률을 기반으로 최소 평균 정보량을 표현한 것이 Entropy
  • H(P) = -(P(n) * log2P(n)의 총합)
즉, 크로스 엔트로피는 Information Gain이 다를 때(예측 확률 분포)를 계산하는 식이다.
  • log2P(n)만 log2Q(n)로 바뀜, H(P) + KL divergence
