Cross Entropy Loss Function

두 확률분포의 차이를 계산하는 손실함수
실제 데이터의 분포를 원핫인코딩으로 표현했을 때와 모델이 계산한 확률의 차이를 계산한다.
H(P, Q) = -(P(n) * logQ(n)의 총합)
H(P, Q) = -((0 * log0.15) + (1 * log0.65) + (0 * log0.20))
  • 앞단에 -를 붙이는 이유는 0~1 사이의 log값은 음수이기 때문(양수로 변환)


Entropy와 Cross Entropy Loss

크로스 엔트로피는 이론적으로 P와 Q 두 확률간의 Entropy를 계산하는 식이다.
  • Entropy : 정보를 표현하는데 있어 필요한 최소 자원량
  • 정보 x를 비트로 표현하기 위해서는 비트 log2x개가 필요함(정보 4개면 비트 2개)
