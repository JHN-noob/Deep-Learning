옵티마이저(Optimizer)
옵티마이저란 가중치(파라미터)의 값을 업데이트해주는 알고리즘이다.
하이퍼 파라미터와 다른 점은 하이퍼 파라미터는 모델 전체에 영향을 준다는 점이다.

1. SGD(Stochastic Gradient Descent)
원래 확률적 경사하강법은 배치 1개만 가지고 가중치를 업데이트하는 방식이지만 보통 실제로는 미니 배치를 업데이트한다.
가장 기본적인 옵티마이저로 미분값을 이용해 손실이 최저점이 되는 곳을 찾으면서 가중치를 업데이트한다.
수식 : w - (학습률 * 기울기)

2. SGD + Momentum
SGD에 모멘텀 계수를 추가한 옵티마이저
모멘텀 계수를 이용해 같은 방향으로 기울기가 반복되면 더 빨리 하강해 로컬 미니마를 탈출할 수 있다.
하지만 단점은 관성으로 인해 오버슈팅되어 반대로 최저점을 탈출할 수도 있다는 점이다. 또한 학습률이 너무 크면 발산할 수 있다.
수식 : w - v
  • v : (모멘텀 계수 * v_t-1) + (학습률 * 기울기)

3. NAG(Nesterov Accelerated Gradent)
모멘텀과 유사하지만 항상 현재 위치에서 기울기를 계산하는 모멘텀과 달리 NAG는 이전 속도만큼 미리 가본 위치에서 기울기를 계산한다.
수렴 속도가 빠르고 안정적이다.
수식 : w - v
  • v : (모멘텀 계수 * v_t-1) + 학습률 * (w - v_t-1 기울기)

4. Adagrad
경사하강법을 기반으로 하는 옵티마이저들이 항상 같은 값의 학습률을 사용하는 것과 달리 각 파라미터마다 다르게 적용하는 방법
파라미터 별로 기울기를 제곱해서 누적한 값을 분모로 주므로 학습이 진행될수록 누적된 값(기울기)이 커지면 분자인 학습률이 작아진다.
하지만 학습이 진행될수록 학습률이 0에 수렴하는 단점이 있다.
수식 : w - (학습률 / (root(기울기 누적) + E)) * 기울기
  • E : 수치적 안정성을 위한 매우 작은 값

5. RMSprop
Adagrad의 학습률이 0으로 수렴하는 문제를 해결하기 위해 나온 옵티마이저
EMA(지수이동평균)을 이용해 최근 스케일만 기울기를 반영하기 때문에 시계열/비정형 데이터에 유리하다.
수식 : w - 학습률 * (기울기 / root(v) + E)
  • v : (p * v_t-1) + (1 - p) * (기울기)^^2
  • p : EMA의 기억 길이(decay)
  • E : 수치적 안정성을 위한 매우 작은 값

6. Adam(Adaptive Moment Estimation)
RMSProp + 모멘텀의 형식이며 모멘텀과 학습률 최적화의 장점을 합친 현재까지 가장 많이 쓰는 옵티마이저
1차적으로 기울기의 이동평균을 잡고 2차적으로 기울기 제곱의 이동평균(분산)으로 학습률을 조정한다. 추가적으로 초기 모멘트가 0이라 편향될 것을 대비해 바이어스 보정도 한다.
수식 : w - 학습률 * (m_t / root(v_t) + E)
  • 1차 모멘트(평균) : m_t = (B_1 * m_t-1) + (1 - B_1) * 기울기
  • 2차 모멘트(제곱평균) : v_t = (B_2 * v_t-1) + (1 - B_2) * (기울기)^^2
  • 바이어스 보정 : m_t = m_t / (1 - (B_1)^^t), v_t = v_t / (1 - (B_2)^^t)
  • E : 수치적 안정성을 위한 매우 작은 값
