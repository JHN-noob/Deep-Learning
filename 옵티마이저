옵티마이저(Optimizer)
옵티마이저란 가중치(파라미터)의 값을 업데이트해주는 알고리즘이다.
하이퍼 파라미터와 다른 점은 하이퍼 파라미터는 모델 전체에 영향을 준다는 점이다.

1. SGD(Stochastic Gradient Descent)
원래 확률적 경사하강법은 배치 1개만 가지고 가중치를 업데이트하는 방식이지만 보통 실제로는 미니 배치를 업데이트한다.
가장 기본적인 옵티마이저로 미분값을 이용해 손실이 최저점이 되는 곳을 찾으면서 가중치를 업데이트한다.
수식 : w - (학습률 x 기울기)

2. SGD + Momentum
SGD에 모멘텀 계수를 추가한 옵티마이저
모멘텀 계수를 이용해 같은 방향으로 기울기가 반복되면 더 빨리 하강해 로컬 미니마를 탈출할 수 있다.
하지만 단점은 관성으로 인해 오버슈팅되어 반대로 최저점을 탈출할 수도 있다는 점이다. 또한 학습률이 너무 크면 발산할 수 있다.
수식 : w - v
  • v : (모멘텀 계수 x vt-1) + (학습률 x 기울기)
