옵티마이저(Optimizer)
옵티마이저란 가중치(파라미터)의 값을 업데이트해주는 알고리즘이다.
하이퍼 파라미터와 다른 점은 하이퍼 파라미터는 모델 전체에 영향을 준다는 점이다.

1. SGD(Stochastic Gradient Descent)
원래 확률적 경사하강법은 배치 1개만 가지고 가중치를 업데이트하는 방식이지만 보통 실제로는 미니 배치를 업데이트한다.
가장 기본적인 옵티마이저로 미분값을 이용해 손실이 최저점이 되는 곳을 찾으면서 가중치를 업데이트한다.
수식 : w - (학습률 * 기울기)

2. SGD + Momentum
SGD에 모멘텀 계수를 추가한 옵티마이저
모멘텀 계수를 이용해 같은 방향으로 기울기가 반복되면 더 빨리 하강해 로컬 미니마를 탈출할 수 있다.
하지만 단점은 관성으로 인해 오버슈팅되어 반대로 최저점을 탈출할 수도 있다는 점이다. 또한 학습률이 너무 크면 발산할 수 있다.
수식 : w - v
  • v : (모멘텀 계수 * v_t-1) + (학습률 * 기울기)

3. NAG(Nesterov Accelerated Gradent)
모멘텀과 유사하지만 항상 현재 위치에서 기울기를 계산하는 모멘텀과 달리 NAG는 이전 속도만큼 미리 가본 위치에서 기울기를 계산한다.
수렴 속도가 빠르고 안정적이다.
수식 : w - v
  • v : (모멘텀 계수 * v_t-1) + 학습률 * (w - v_t-1 기울기)

4. Adagrad
경사하강법을 기반으로 하는 옵티마이저들이 항상 같은 값의 학습률을 사용하는 것과 달리 각 파라미터마다 다르게 적용하는 방법
파라미터 별로 기울기를 제곱해서 누적한 값을 분모로 주므로 학습이 진행될수록 누적된 값(기울기)이 커지면 분자인 학습률이 작아짐
수식 : w - (학습률 / (root(기울기 누적) + E)) * 기울기
  • E : 수치적 안정성을 위한 작은 값
