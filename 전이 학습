전이 학습(Transfer Learning)


전이 학습이란 특정 태스트를 학습한 모델을 다른 태스크 수행에 재활용하는 기법
컴퓨터 비전 분야를 필두로 전이학습 사용 결과, 기존 지도학습 모델보다 개선된 성능을 보였다.

Transfer Learning 학습
일반적으로 대량의 데이터를 학습한 사전 학습 모델(Pretrained Model or Backbone)을 사용한다.
Pre-trained Model에서 출력을 뽑아내는 일을 Feature Map을 뽑아낸다고 한다. 또한 해당 Feature들을 Bottleneck Feature라고 부른다.

Transfer Learning의 4가지 분류
Small dataset and similiar to the pre-trained model's dataset
  • pre-trained model을 포함한 전체 네트워크를 학습시키면 overfitting의 위험이 있다.
  • pre-trained model은 freeze하고, pre-trained model 마지막에 FC layer를 넣어서 해당 FC layer만 학습 추천
Small dataset and different from the pre-trained model's dataset
  • 이 경우는 마지막에 FC layer만 학습시키는것은 좋지않다.
  • pre-trained model의 일부를 포함해서 이후 layer들을 모두 학습하는것을 추천
Large dataset, and similiar to the pre-trained model's dataset
  • 새로 학습시킬 데이터셋이 많다면 overfitting의 위험이 낮으므로,
  • pre-trained model을 포함 전체 네트워크를 학습시키거나,
  • pre-trained model의 일부를 포함해서 이후 layer들을 모두 학습시키거나,
  • 마지막 일부 layer들을 dropout하는 등의 다양한 방식 추천
Large dataset, and different from the pre-trained model's dataset
  • 새로 학습시킬 데이터셋이 많다면 overfitting의 위험이 낮으므로, pre-trained model을 포함한 전체 네트워크 학습 추천
※ pre-trained model의 파라미터까지 학습시에는 일반적으로 learning rate를 1/10 작게 한다.

Fine Tuning과 Transfer Learning
Transfer Learning
  • 입력층에 가까운 파라미터일수록 변화하지않고 사용하는 방식
  • pre-trained model을 기반으로 최종 출력층을 추가 또는 변경하여 학습하는 방식
  • pre-trained model에 변경 또는 추가한 최종 출력층의 파라미터를 small dataset으로 학습시키는 방식
Fine Tuning
  • 이미 만들어져있는 model에 변겨 ㅇ또는 일부 레이어를 추가하여 전체 네트워크를 구성한 후,
  • 직접 준비한 데이터로 전체 네트워크의 모든 파라미터를 재학습하는 방식
※ Fine Tuning 및 Transfer Learning에서 일부 레이어에 대한 가중치 업데이트시에는 Adam보다 SGD를 많이 사용한다.

전이 학습의 효과
전이 학습은 실제로 학습 속도를 높여주는데 효과가 있다.
모델을 불러와서 사용하므로 코드 작성을 줄여준다.
