{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101a3001-17ae-4d3a-960a-48edd443c91f",
   "metadata": {},
   "source": [
    "# Convolution Layers와 PyTorch\n",
    "- Conv1d(1차원, Text-CNN에서 많이 사용)\n",
    "- Conv2d(2차원, 이미지 분류에서 많이 사용\n",
    "- Conv3d(3차원)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695473d-5cb8-4b90-823d-63ae80ebebee",
   "metadata": {},
   "source": [
    "# Conv2d\n",
    "- Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ecf90e-6d7a-4132-b8b5-53c86b180d2c",
   "metadata": {},
   "source": [
    "# Shape 이해\n",
    "- Input Tensor: (N, C, H, W)\n",
    "  - N: batch 사이즈\n",
    "  - C: in_channels와 일치해야함\n",
    "  - H: 2D input Tensor의 높이\n",
    "  - W: 2D input Tensor의 너비\n",
    "- Output Tensor: (N, C, H, W)\n",
    "  - N: batch 사이즈\n",
    "  - C: out_channels와 일치해야함\n",
    "  - H: 출력높이 수식\n",
    "  - W: 출력너비 수식\n",
    "    - 다양한 CNN 알고리즘 중에는 너비, 높이에서의 padding, stride를 달리할 수 있고, 이를 stride[0], stride[1]과 같은 식의 튜플 형태로 적용도 가능함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e851d2-52b3-46f9-b88a-ff24f36356fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv1 = nn.Conv2d(1, 1, 3, padding=1)\n",
    "input1 = torch.Tensor(1, 1, 5, 5)\n",
    "out1 = conv1(input1)\n",
    "out1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019390bb-4efb-4219-858f-d0bbf18fcdfe",
   "metadata": {},
   "source": [
    "# Pooling Layers\n",
    "- 입력 데이터 차원에 맞추어 Max Pooling 또는 Average Pooling을 적용할 수 있음\n",
    "  - MaxPool1d\n",
    "  - MaxPool2d\n",
    "  - MaxPool3d\n",
    "  - AvgPool1d\n",
    "  - AvgPool2d\n",
    "  - AvgPool3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dec87d-1e90-490b-88a8-c3edae12979e",
   "metadata": {},
   "source": [
    "# MaxPool2d\n",
    "- MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9064525b-ee02-4402-b89e-88b1e4894de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5, 5])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv1 = nn.Conv2d(1, 1, 3, padding=1)\n",
    "input1 = torch.Tensor(1, 1, 5, 5)\n",
    "pool1 = nn.MaxPool2d(2)\n",
    "out1 = conv1(input1)\n",
    "print(out1.shape)\n",
    "out2 = pool1(out1)\n",
    "print(out2.shape) # 5/2와 같은 실수면 default는 무조건 내림(5/2 = 2)이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7816c01c-2cc8-44f0-8ea4-29c5cda2ee13",
   "metadata": {},
   "source": [
    "# 모델 정의\n",
    "- Convolution Layer는 입력 데이터에 필터(커널) 적용 후, 활성화함수를 적용한 Layer를 의미함\n",
    "  1. Convolution Layer는 입력 데이터에 필터(커널) 적용을 위한 전용 클래스 제공(nn.Conv2d)\n",
    "  2. 이후에 활성화함수 적용(예: nn.LeakyReLU(0.1))\n",
    "  3. 이후에 Batch Normalization, Dropout 등 regularization 적용(옵션)\n",
    "  4. 이후에 Pooling 적용(예: nn.MaxPool2d)\n",
    "- BatchNorm1d()와 BatchNorm2d()\n",
    "  - BatchNorm1d(C)는 Input과 Output이 (N, C) 또는 (N, C, L)의 형태\n",
    "    - N은 Batch 크기, C는 Channel, L은 Lengh\n",
    "  - BatchNorm2d(C)는 Input과 Output이 (N, C, H, W)의 형태\n",
    "    - N은 Batch 크기, C는 Channel, H는 Height, W는 Width\n",
    "  - 인자로 Output Channel 수를 넣으면 되며, Conv2d()에서는 BatchNorm2d()를 사용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a303065c-4d14-44cc-8ca8-756c8aec9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(2)\n",
    "    # Img = (?, 1, 28, 28)\n",
    "    # 출력 = (?, 32, 28/2, 28/2)\n",
    ")\n",
    "input1 = torch.Tensor(1, 1, 28, 28)\n",
    "out1 = conv1(input1)\n",
    "print(out1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1df60a8-860b-4a64-8ee4-500bd59b0a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 3, 3]) torch.Size([1, 1152]) 1152\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.MaxPool2d(2),\n",
    "    # 출력 = (1, 32, 14, 14)\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.MaxPool2d(2),\n",
    "    # 출력 = (1, 64, 7, 7)\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm2d(128),\n",
    "    nn.MaxPool2d(2)\n",
    "    # 출력 = (1, 128, 3, 3)\n",
    ")\n",
    "\n",
    "input1 = torch.Tensor(1, 1, 28, 28)\n",
    "out1 = conv1(input1)\n",
    "out2 = out1.view(out1.size(0), -1)\n",
    "print(out1.shape, out2.shape, 128*3*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82416fe8-6eaf-446f-8f86-7240158eb694",
   "metadata": {},
   "source": [
    "# CNN 모델 구성\n",
    "1. 다음 세트로 하나의 Convolution Layer + Pooling Layer를 구성하고 여러 세트로 구축\n",
    "    - 보통 Convolution Layer + Pooling Layer의 출력 채널을 늘리는 방식으로 여러 세트 구축\n",
    "2. Flatten\n",
    "    - 텐서.view(텐서.size(0), -1)로 Flatten\n",
    "3. 여러 Fully-Connected Layer 구성\n",
    "    - Flatten한 입력을 받아서 최종 Multi-Class 개수만큼 출력\n",
    "    - Multi-Class일 경우, nn.LogSoftmax()로 최종 결과값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40439438-3849-4865-8f2b-665285e0e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            # 출력 = (N, 32, 14, 14)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            # 출력 = (N, 64, 7, 7)\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2)\n",
    "            # 출력 = (N, 128, 3, 3)\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(128*3*3, 128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, 10),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afdc78-e332-4827-a80a-a566e1340efe",
   "metadata": {},
   "source": [
    "# MNIST with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db790bd1-cf95-42ae-b425-b52e51e96013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2d4356-1e92-4254-b7ea-cd9d25b661f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data :  60000\n",
      "number of test data :  10000\n"
     ]
    }
   ],
   "source": [
    "train_rawdata = datasets.MNIST(root = 'dataset_MNIST',\n",
    "                               train = True, # True면 Train 데이터\n",
    "                               download = True, # 데이터가 없으면 Download\n",
    "                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌\n",
    "test_rawdata = datasets.MNIST(root = 'dataset_MNIST',\n",
    "                               train = False, # False면 Test 데이터\n",
    "                               download = True, # 데이터가 없으면 Download\n",
    "                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌\n",
    "print('number of training data : ', len(train_rawdata))\n",
    "print('number of test data : ', len(test_rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "621a61e7-d2d6-4ead-aca2-818dfce5c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_RATE = 0.2\n",
    "train_indices, val_indices, _, _ = train_test_split(\n",
    "    range(len(train_rawdata)), # Train 데이터셋의 인덱스 번호 추출(0~59999)\n",
    "    train_rawdata.targets, # y 정답 라벨\n",
    "    stratify = train_rawdata.targets, # y 정답 라벨 균등분포\n",
    "    test_size = VALIDATION_RATE # 여기선 Validation 데이터셋 비율\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1544a3-c07b-4646-bb85-bd51ae79f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Subset(train_rawdata, train_indices)\n",
    "validation_dataset = Subset(train_rawdata, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cc82bb5-7a0d-4684-b775-927e6125cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 12000 10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(validation_dataset), len(test_rawdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b9f179-8ad8-439c-a3fe-bd6023b8f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "train_batchs = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "va_batchs = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_batchs = DataLoader(test_rawdata, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77113b38-184c-41a9-8955-239260372711",
   "metadata": {},
   "source": [
    "# CNNModel 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc696b2b-2de3-4e56-a5c1-bc7e85cba59f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): LeakyReLU(negative_slope=0.1)\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): LeakyReLU(negative_slope=0.1)\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linear_layers): Sequential(\n",
       "    (0): Linear(in_features=1152, out_features=128, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.1)\n",
       "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (4): LeakyReLU(negative_slope=0.1)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (7): LogSoftmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b5e100-d58b-48c1-a167-5a38a2c02098",
   "metadata": {},
   "source": [
    "# loss, optimizer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330c6eb5-b389-4c98-a9ff-2d196ac73b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82669f04-3196-4960-809f-9b606d29fb29",
   "metadata": {},
   "source": [
    "# Training & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fbec2d9-5198-441a-a2c6-87788850036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, early_stop, nb_epochs, progress_interval):\n",
    "    train_losses, valid_losses, lowest_loss = list(), list(), np.inf\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        train_loss, valid_loss = 0, 0\n",
    "\n",
    "        # train model\n",
    "        model.train() # prepare model for training\n",
    "        for x_minibatch, y_minibatch in train_batchs:\n",
    "            y_minibatch_pred = model(x_minibatch)\n",
    "            loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_batchs)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validate model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x_minibatch, y_minibatch in va_batchs:\n",
    "                y_minibatch_pred = model(x_minibatch)\n",
    "                loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "                valid_loss += loss.item()\n",
    "    \n",
    "        valid_loss = valid_loss / len(va_batchs)\n",
    "        valid_losses.append(valid_loss)\n",
    "    \n",
    "        if valid_losses[-1] < lowest_loss:\n",
    "            lowest_loss = valid_losses[-1]\n",
    "            lowest_epoch = epoch\n",
    "            best_model = deepcopy(model.state_dict())\n",
    "        else:\n",
    "            if (early_stop > 0) and lowest_epoch + early_stop < epoch:\n",
    "                print(\"Early Stopped\", epoch, \"epochs\")\n",
    "                break\n",
    "    \n",
    "        if (epoch % progress_interval) == 0:\n",
    "            print(train_losses[-1], valid_losses[-1], lowest_loss, lowest_epoch, epoch)\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, lowest_loss, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f416d-8be4-4354-afd8-31773420800f",
   "metadata": {},
   "source": [
    "# 훈련 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c869d8b3-35a6-4da2-aaf7-8e37b95e7bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12998725000272193 0.05393193105354588 0.05393193105354588 0 0\n",
      "0.01516358590591699 0.029974613274308913 0.029974613274308913 3 3\n",
      "0.009930346643474574 0.03491437948492177 0.029974613274308913 3 6\n",
      "0.005683745020534843 0.0470760349980605 0.029974613274308913 3 9\n",
      "0.004945649953229198 0.0491833004057395 0.029974613274308913 3 12\n",
      "Early Stopped 14 epochs\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 30\n",
    "progress_interval = 3\n",
    "early_stop = 10\n",
    "\n",
    "model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, nb_epochs, progress_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
