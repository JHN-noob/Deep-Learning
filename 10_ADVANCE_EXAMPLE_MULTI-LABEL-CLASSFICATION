import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from sklearn.model_selection import train_test_split
from copy import deepcopy
import numpy as np


train_rawdata = datasets.MNIST(root = 'dataset_MNIST',
                               train = True, # True면 Train 데이터
                               download = True, # 데이터가 없으면 Download
                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌
test_rawdata = datasets.MNIST(root = 'dataset_MNIST',
                               train = False, # False면 Test 데이터
                               download = True, # 데이터가 없으면 Download
                               transform = transforms.ToTensor()) # raw 포맷을 텐서로 바꿔줌
print('number of training data : ', len(train_rawdata))
print('number of test data : ', len(test_rawdata))

# Validation dataset 분리
VALIDATION_RATE = 0.2
train_indices, val_indices, _, _ = train_test_split(
    range(len(train_rawdata)), # Train 데이터셋의 인덱스 번호 추출(0~59999)
    train_rawdata.targets, # y 정답 라벨
    stratify = train_rawdata.targets, # y 정답 라벨 균등분포
    test_size = VALIDATION_RATE # 여기선 Validation 데이터셋 비율
)

# torch.utils.data.Subset으로 Validation dataset 생성
train_dataset = Subset(train_rawdata, train_indices) # train_rawdata에 train_indices(인덱스 번호)가 추가됨
validation_dataset = Subset(train_rawdata, val_indices) # train_rawdata에 val_indices(인덱스 번호) 20%가 추가됨
print(len(train_dataset), len(validation_dataset), len(test_rawdata))

# Mini-batch 생성
BATCH_SIZE = 128
train_batchs = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
va_batchs = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_batchs = DataLoader(test_rawdata, batch_size=BATCH_SIZE, shuffle=False)

# 시각화(참고)
X_train, y_train = next(iter(train_batchs))
print(X_train.shape, y_train.shape)

X_train[0, :, :, :].shape

X_train[0, :, :, :].numpy().reshape(28, 28).shape

# 미니배치 데이터 중 100개 이미지만 출력해보기
import matplotlib.pyplot as plt
# 주피터 노트북에서 그림을 노트북 내에 표시하도록 강제하는 명령
%matplotlib inline

plt.figure(figsize=(10, 12)) # 표시할 그림의 사이즈를 정의

X_train, y_train = next(iter(train_batchs))
for index in range(100):
    plt.subplot(10, 10, index+1) # 표시할 그림들의 행렬 및 시작위치를 정의
    plt.axis('off') # 축 off
    plt.imshow(X_train[index, :, :, :].numpy().reshape(28, 28), cmap = 'gray')
    # 미니배치 X_train을 첫번째 차원(인덱스) 기준으로 ndarray로 변환시키고 그걸 다시 (28, 28)로 reshape한다, 색깔은 gray
    plt.title('Class: ' + str(y_train[index].item()))
    
plt.show()

# 모델 정의
X_train, y_train = next(iter(train_batchs))
print(X_train.shape, y_train.shape)
print(X_train.size(0))
print(X_train.view(X_train.size(0), -1).shape)
# 학습 데이터를 모델 입력에 맞게 변환하는 과정이 필요한데, view 함수를 써서 미니배치 [128, 1, 28, 28] 텐서를 [128, 784] 텐서(2차원)로 변환한다.

class FunModel(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()

        self.linear_layers = nn.Sequential(
            nn.Linear(input_dim, 256),
            nn.LeakyReLU(0.1),
            nn.Linear(256, 256),
            nn.LeakyReLU(0.1),
            nn.Linear(256, 128),
            nn.LeakyReLU(0.1),
            nn.Linear(128, output_dim),
            nn.LogSoftmax(dim=-1)
        )

    def forward(self, x):
        y = self.linear_layers(x)
        return y

minibatch_size = 128
input_dim = 28*28
output_dim = 10
model = FunModel(input_dim, output_dim)

loss_func = nn.NLLLoss()
optimizer = torch.optim.Adam(model.parameters())

# 베스트 모델 저장
# Training Cdoe 템플릿
- model : DNN 모델
- early_stop : Early Stopping을 위해, 얼마까지의 validation set에 대한 loss가 낮아지지 않을 경우 Early Stopping을 할지의 하이퍼 파라미터 값
- nb_epochs : 최대 epoch
- progress_interval : 진행상황을 얼마의 epoch마다 출력할지의 epoch 값

def train_model(model, early_stop, nb_epochs, progress_interval):
    train_losses, valid_losses, lowest_loss = list(), list(), np.inf

    for epoch in range(nb_epochs):
        train_loss, valid_loss = 0, 0

        # train model
        model.train() # prepare model for training
        for x_minibatch, y_minibatch in train_batchs:
            y_minibatch_pred = model(x_minibatch.view(x_minibatch.size(0), -1))
            loss = loss_func(y_minibatch_pred, y_minibatch)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        train_loss = train_loss / len(train_batchs)
        train_losses.append(train_loss)

        # validate model
        model.eval()
        with torch.no_grad():
            for x_minibatch, y_minibatch in va_batchs:
                y_minibatch_pred = model(x_minibatch.view(x_minibatch.size(0), -1))
                loss = loss_func(y_minibatch_pred, y_minibatch)
                valid_loss += loss.item()
    
        valid_loss = valid_loss / len(va_batchs)
        valid_losses.append(valid_loss)
    
        if valid_losses[-1] < lowest_loss:
            lowest_loss = valid_losses[-1]
            lowest_epoch = epoch
            best_model = deepcopy(model.state_dict())
        else:
            if (early_stop > 0) and lowest_epoch + early_stop < epoch:
                print("Early Stopped", epoch, "epochs")
                break
    
        if (epoch % progress_interval) == 0:
            print(train_losses[-1], valid_losses[-1], lowest_loss, lowest_epoch, epoch)

    model.load_state_dict(best_model)
    return model, lowest_loss, train_losses, valid_losses

nb_epochs = 50
progress_interval = 10
early_stop = 5

model, lowest_loss, train_losses, valid_losses = train_model(model, early_stop, nb_epochs, progress_interval)

valid_losses
