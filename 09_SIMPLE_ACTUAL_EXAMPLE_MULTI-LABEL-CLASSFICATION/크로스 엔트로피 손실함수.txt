Cross Entropy Loss Function

두 확률분포의 차이를 계산하는 손실함수
실제 데이터의 분포를 원핫인코딩으로 표현했을 때와 모델이 계산한 확률의 차이를 계산한다.
H(P, Q) = -(P(n) * logQ(n)의 총합)
H(P, Q) = -((0 * log0.15) + (1 * log0.65) + (0 * log0.20))
  • 앞단에 -를 붙이는 이유는 0~1 사이의 log값은 음수이기 때문(양수로 변환)


Entropy와 Cross Entropy Loss

크로스 엔트로피는 이론적으로 P와 Q 두 확률간의 Entropy를 계산하는 식이다.
  • Entropy : 정보를 표현하는데 있어 필요한 최소 자원량
  • 정보 x를 비트로 표현하기 위해서는 비트 log2x개가 필요함(정보 4개면 비트 2개)
만약 특정 정보가 나올 확률이 확연히 높다면 해당 정보는 작은 비트수를 할당하고 낮은 정보는 큰 비트수를 할당한다.
각 정보가 나올 확률을 기반으로 최소 평균 정보량을 표현한 것이 Entropy
  • H(P) = -(P(n) * log2P(n)의 총합)
즉, 크로스 엔트로피는 Information Gain이 다를 때(예측 확률 분포)를 계산하는 식이다.
  • log2P(n)만 log2Q(n)로 바뀜, H(P) + KL divergence


Metric
  • Precision과 Recall
                                       실제
                              바이러스   바이러스 아님
예   바이러스 진단                TP          FP
측   바이러스 아닌것으로 진단      FN          TN
Recall(민감도) : 실제 병에 걸린 사람이 양성으로 판정받을 비율, TP / (TP + FN)
Precision(정밀도) : 양성으로 판정했을때 실제로 병에 걸린 비율, TP / (TP + FP)
F1 Score : Precision과 Recall을 결합한 지표, 어느 한쪽으로 치우치지않을 경우 상대적으로 높은 값이 나옴, 2 * ((P * R) / (P + R))
Specificity, True Negative Rate(TNR)(특이도)
Fasle Positive Rate(FPR)
Confusion Matrix : 다중 분류일 경우 각 분류에 대한 Metric을 표로 나타낸것
Micro metric : 전체 라벨값을 합하여 계산함
Macro metric : 라벨별로 계산된 값에 대한 전체 평균을 출력함
